# Biases-In-AI-Based-Risk-Estimation
This paper examines biases in AI-based risk estimation, focusing on the ethical implications of recidivism prediction systems. It emphasizes the need to balance fairness, accuracy, and transparency while addressing biases that disproportionately affect vulnerable groups.

The study highlights progress in de-biasing techniques, such as removing sensitive features (e.g., age, sex, race) and applying PCA to reduce indirect biases and noise. These methods improve equity and fairness, offering a pathway toward more ethical applications in criminal justice.

However, significant challenges remain. While debiased models showed improved consistency, the overlap between high-risk and low-risk classes in PCA-reduced feature spaces underscores difficulties in making accurate distinctions. Residual historical biases in non-protected features and systemic inequalities in training data further risk perpetuating inequities. Additionally, the opacity of AI predictions raises concerns about transparency and accountability, increasing the likelihood of automation bias if stakeholders overtrust algorithmic outcomes.

The findings underscore the importance of pairing technical advancements with oversight mechanisms, interpretability, and frameworks for addressing unintended harms. Without these safeguards, the risks of amplifying existing biases and inequalities could outweigh the benefits of using AI in recidivism prediction.
